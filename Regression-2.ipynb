{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a763716-f9e4-4156-8a72-2b34162a54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. R-squared in Linear Regression Models\n",
    "Concept:\n",
    "•\tR-squared (Coefficient of Determination): Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "Calculation:\n",
    "•\tR2=1−SSresSStotR^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}R2=1−SStotSSres\n",
    "o\tSSres_{\\text{res}}res: Sum of squares of residuals (errors).\n",
    "o\tSStot_{\\text{tot}}tot: Total sum of squares (variance of the dependent variable).\n",
    "Representation:\n",
    "•\t0 ≤ R² ≤ 1: R-squared ranges from 0 to 1, where 1 indicates that the model explains all the variability of the response data around its mean, and 0 indicates that the model explains none of the variability.\n",
    "Q2. Adjusted R-squared\n",
    "Definition:\n",
    "•\tAdjusted R-squared: A modified version of R-squared that adjusts for the number of predictors in the model.\n",
    "Calculation:\n",
    "•\tAdjusted R2=1−(1−R2n−p−1)×(n−1)\\text{Adjusted } R^2 = 1 - \\left(\\frac{1 - R^2}{n - p - 1}\\right) \\times (n - 1)Adjusted R2=1−(n−p−11−R2)×(n−1)\n",
    "o\tn: Number of observations.\n",
    "o\tp: Number of predictors.\n",
    "Difference:\n",
    "•\tR-squared: Can be artificially high with more predictors.\n",
    "•\tAdjusted R-squared: Accounts for the number of predictors, making it a better measure for models with multiple predictors.\n",
    "Q3. When to Use Adjusted R-squared\n",
    "•\tAppropriate When: Comparing models with different numbers of predictors.\n",
    "•\tWhy: Adjusted R-squared adjusts for the number of predictors and provides a more accurate measure of model performance, especially when adding predictors does not necessarily improve the model.\n",
    "Q4. RMSE, MSE, and MAE in Regression Analysis\n",
    "RMSE (Root Mean Squared Error):\n",
    "•\tDefinition: The square root of the average squared differences between predicted and actual values.\n",
    "•\tCalculation: RMSE=1n∑i=1n(yi−y^i)2\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}RMSE=n1∑i=1n(yi−y^i)2\n",
    "•\tRepresentation: Measures the average magnitude of errors, with higher weight for larger errors.\n",
    "MSE (Mean Squared Error):\n",
    "•\tDefinition: The average of the squared differences between predicted and actual values.\n",
    "•\tCalculation: MSE=1n∑i=1n(yi−y^i)2\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2MSE=n1∑i=1n(yi−y^i)2\n",
    "•\tRepresentation: Measures the average squared magnitude of errors.\n",
    "MAE (Mean Absolute Error):\n",
    "•\tDefinition: The average of the absolute differences between predicted and actual values.\n",
    "•\tCalculation: MAE=1n∑i=1n∣yi−y^i∣\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|MAE=n1∑i=1n∣yi−y^i∣\n",
    "•\tRepresentation: Measures the average magnitude of errors without squaring them.\n",
    "Q5. Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "RMSE:\n",
    "•\tAdvantages: Penalizes larger errors more heavily, useful when large errors are particularly undesirable.\n",
    "•\tDisadvantages: Sensitive to outliers due to squaring of errors.\n",
    "MSE:\n",
    "•\tAdvantages: Easy to compute and differentiate, sensitive to large errors.\n",
    "•\tDisadvantages: Sensitive to outliers, as it squares the errors.\n",
    "MAE:\n",
    "•\tAdvantages: Less sensitive to outliers compared to RMSE and MSE, provides a straightforward interpretation of average error.\n",
    "•\tDisadvantages: Does not penalize larger errors as much as RMSE, which may be less useful in some contexts.\n",
    "Q6. Lasso Regularization\n",
    "Concept:\n",
    "•\tLasso (Least Absolute Shrinkage and Selection Operator): Adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.\n",
    "•\tRegularization Term: λ∑j=1p∣βj∣\\lambda \\sum_{j=1}^p |\\beta_j|λ∑j=1p∣βj∣\n",
    "Difference from Ridge Regularization:\n",
    "•\tLasso: Can shrink some coefficients to zero, effectively performing feature selection.\n",
    "•\tRidge: Shrinks all coefficients but does not set any to zero.\n",
    "When to Use:\n",
    "•\tLasso: When feature selection is desired or when the model has many predictors, and some may be irrelevant.\n",
    "Q7. Regularized Linear Models and Overfitting\n",
    "Concept:\n",
    "•\tRegularized Models: Add a penalty to the loss function to reduce the complexity of the model and prevent overfitting.\n",
    "Example:\n",
    "•\tRidge Regularization: Adds a penalty proportional to the sum of the squared coefficients.\n",
    "•\tLasso Regularization: Adds a penalty proportional to the sum of the absolute values of coefficients.\n",
    "How It Helps:\n",
    "•\tBy penalizing large coefficients, regularization reduces the model’s variance, making it less likely to overfit the training data.\n",
    "Q8. Limitations of Regularized Linear Models\n",
    "Limitations:\n",
    "•\tChoice of Regularization Parameter: Selecting the optimal regularization parameter can be challenging and requires cross-validation.\n",
    "•\tModel Interpretability: Regularization, especially Lasso, can make it difficult to interpret the model if many coefficients are shrunk to zero.\n",
    "•\tNot Always Suitable: For models where non-linearity or interactions between features are critical.\n",
    "Q9. Comparing Models with RMSE and MAE\n",
    "Models:\n",
    "•\tModel A (RMSE = 10)\n",
    "•\tModel B (MAE = 8)\n",
    "Choosing the Better Model:\n",
    "•\tMAE (Model B): Indicates that the average magnitude of errors is lower. It might be preferred if you are more concerned with the average error size without squaring.\n",
    "Limitations:\n",
    "•\tRMSE: Sensitive to outliers, may be higher due to a few large errors.\n",
    "•\tMAE: Provides a more robust measure when dealing with outliers.\n",
    "Q10. Comparing Regularized Models\n",
    "Models:\n",
    "•\tModel A (Ridge Regularization, λ = 0.1)\n",
    "•\tModel B (Lasso Regularization, λ = 0.5)\n",
    "Choosing the Better Model:\n",
    "•\tRidge: If you want to keep all features but reduce their impact.\n",
    "•\tLasso: If feature selection is important and some features should be completely excluded.\n",
    "Trade-offs:\n",
    "•\tRidge: Does not perform feature selection, all features remain in the model.\n",
    "•\tLasso: Can eliminate features, potentially simplifying the model but might miss some useful features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
